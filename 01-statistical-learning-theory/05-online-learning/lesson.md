# Online Learning and Regret Minimization

## Prerequisites
- Convex optimization (gradients, projections)
- Probability theory (concentration inequalities)
- Game theory basics

## Learning Objectives
- Master online learning framework and regret bounds
- Understand online-to-batch conversion
- Implement efficient online algorithms
- Connect to adversarial machine learning

## Mathematical Foundations

### 1. Online Learning Framework

#### Protocol
For T rounds:
1. Learner chooses action x‚Çú ‚àà ùí≥
2. Environment reveals loss function ‚Ñì‚Çú: ùí≥ ‚Üí ‚Ñù
3. Learner suffers loss ‚Ñì‚Çú(x‚Çú)

#### Adversarial Setting
- Environment can be adversarial
- No distributional assumptions
- Worst-case analysis

#### Examples
- **Online classification**: Predict labels, observe true labels
- **Online convex optimization**: Choose point, observe convex function
- **Bandit problems**: Choose arm, observe reward for chosen arm only

### 2. Regret Measures

#### Definition 2.1 (Regret)
Regret with respect to comparator u ‚àà ùí≥:
R_T(u) = ‚àë‚Çú‚Çå‚ÇÅ·µÄ ‚Ñì‚Çú(x‚Çú) - ‚àë‚Çú‚Çå‚ÇÅ·µÄ ‚Ñì‚Çú(u)

#### Definition 2.2 (Worst-case Regret)
R_T = max_u‚ààùí≥ R_T(u) = ‚àë‚Çú‚Çå‚ÇÅ·µÄ ‚Ñì‚Çú(x‚Çú) - min_u‚ààùí≥ ‚àë‚Çú‚Çå‚ÇÅ·µÄ ‚Ñì‚Çú(u)

#### Regret Minimization
Algorithm is *no-regret* if R_T/T ‚Üí 0 as T ‚Üí ‚àû.

### 3. Online Gradient Descent

#### Algorithm
For convex constraint set ùí≥:
1. Choose x‚ÇÅ ‚àà ùí≥ arbitrarily
2. For t = 1, ..., T:
   - Play x‚Çú, observe ‚Ñì‚Çú
   - Update: x‚Çú‚Çä‚ÇÅ = Œ†_ùí≥(x‚Çú - Œ∑‚Çú‚àá‚Ñì‚Çú(x‚Çú))

where Œ†_ùí≥ is projection onto ùí≥.

#### Theorem 3.1 (OGD Regret Bound)
For convex losses with ||‚àá‚Ñì‚Çú(x)|| ‚â§ G and diam(ùí≥) ‚â§ D:
R_T ‚â§ (D¬≤/2Œ∑) + (Œ∑GT)/2

**Optimal choice**: Œ∑ = D/(G‚àöT) gives R_T ‚â§ DG‚àöT.

**Proof Sketch**:
1. Use projection property: ||x‚Çú‚Çä‚ÇÅ - u||¬≤ ‚â§ ||x‚Çú - Œ∑‚Çú‚àá‚Ñì‚Çú(x‚Çú) - u||¬≤
2. Expand and use convexity: ‚Ñì‚Çú(x‚Çú) - ‚Ñì‚Çú(u) ‚â§ ‚àá‚Ñì‚Çú(x‚Çú)·µÄ(x‚Çú - u)
3. Telescope the bound ‚ñ°

### 4. Follow the Regularized Leader

#### Algorithm (FTRL)
Choose x‚Çú‚Çä‚ÇÅ = argmin_{x‚ààùí≥} [‚àë‚Çõ‚Çå‚ÇÅ·µó ‚Ñì‚Çõ(x) + R(x)]

where R(x) is a regularization function.

#### Examples
- **L2 regularization**: R(x) = (1/2Œ∑)||x||¬≤
- **Entropy regularization**: R(x) = Œ∑‚àë·µ¢ x·µ¢ log x·µ¢ (for simplex)

#### Theorem 4.1 (FTRL Regret Bound)
For Œ±-strongly convex regularizer R:
R_T ‚â§ (R(u) - R(x‚ÇÅ))/Œ± + ‚àë‚Çú‚Çå‚ÇÅ·µÄ‚Åª¬π (‚àá‚Ñì‚Çú(x‚Çú‚Çä‚ÇÅ) - ‚àá‚Ñì‚Çú(x‚Çú))·µÄ(x‚Çú‚Çä‚ÇÅ - x‚Çú)

### 5. Multiplicative Weights / Hedge

#### Setting
Actions are probability distributions over n experts.
Loss is linear: ‚Ñì‚Çú(p) = p·µÄl‚Çú where l‚Çú ‚àà [0,1]‚Åø.

#### Algorithm
Initialize w‚ÇÅ·µ¢ = 1 for all i.
For t = 1, ..., T:
1. Play p‚Çú·µ¢ = w‚Çú·µ¢/‚àë‚±º w‚Çú‚±º
2. Observe losses l‚Çú
3. Update: w‚Çú‚Çä‚ÇÅ,·µ¢ = w‚Çú·µ¢ exp(-Œ∑l‚Çú·µ¢)

#### Theorem 5.1 (Hedge Regret Bound)
R_T ‚â§ (log n)/Œ∑ + Œ∑T/8

**Optimal choice**: Œ∑ = ‚àö(8 log n/T) gives R_T ‚â§ ‚àö(T log n/2).

### 6. Online-to-Batch Conversion

#### Theorem 6.1 (Online-to-Batch)
If online algorithm has regret R_T, then for iid data:
E[f(xÃÑ) - min_u f(u)] ‚â§ R_T/T

where xÃÑ = (1/T)‚àë‚Çú x‚Çú and f(x) = E[‚Ñì(x, z)].

This connects online regret to generalization in stochastic setting.

### 7. Bandit Learning

#### Multi-Armed Bandits
- K arms with unknown reward distributions
- At each time, pull one arm and observe reward
- Goal: Minimize cumulative regret

#### UCB Algorithm
Choose arm with highest upper confidence bound:
a‚Çú = argmax_i [ŒºÃÇ·µ¢ + ‚àö(2 log t/n·µ¢)]

where ŒºÃÇ·µ¢ is empirical mean and n·µ¢ is number of pulls.

#### Theorem 7.1 (UCB Regret)
R_T ‚â§ 8‚àë·µ¢‚Çå‚ÇÇ·¥∑ (log T)/Œî·µ¢ + (1 + œÄ¬≤/3)‚àë·µ¢‚Çå‚ÇÇ·¥∑ Œî·µ¢

where Œî·µ¢ is gap between optimal and i-th arm.

### 8. Contextual Bandits

#### Setting
- Context x‚Çú ‚àà ùí≥ revealed at time t
- Choose action a‚Çú ‚àà ùíú
- Observe reward r(x‚Çú, a‚Çú) + noise

#### LinUCB Algorithm
Assume linear rewards: E[r(x,a)] = x·µÄŒ∏‚Çê
Maintain confidence sets for Œ∏‚Çê and choose optimistically.

#### Thompson Sampling
Maintain posterior over parameters, sample, and act according to sample.

### 9. Adversarial Examples Connection

#### Adversarial Training as Online Learning
- Learner chooses model parameters
- Adversary chooses perturbations
- Minimax formulation leads to online algorithms

#### Robust Optimization
min_Œ∏ max_{||Œ¥||‚â§Œµ} E[‚Ñì(f_Œ∏(x + Œ¥), y)]

Can be solved using online learning techniques.

### 10. Advanced Topics

#### Adaptive Regret
Regret over any interval [s,t]:
R_{s,t} = ‚àë_{u=s}^t ‚Ñì·µ§(x·µ§) - min_x ‚àë_{u=s}^t ‚Ñì·µ§(x)

#### Strongly Convex Losses
For Œº-strongly convex losses:
R_T = O(log T)

Much better than general convex case.

#### Non-convex Online Learning
Recent work on online learning for non-convex problems:
- Stationary point guarantees
- Local regret bounds
- Connections to SGD

### 11. Practical Applications

#### Online Advertising
- Ad placement decisions
- Revenue optimization
- Real-time bidding

#### Recommendation Systems
- Online collaborative filtering
- Exploration vs exploitation
- Cold start problems

#### Portfolio Management
- Online portfolio selection
- Risk management
- Transaction costs

## Implementation Details

See `exercise.py` for implementations of:
1. Online gradient descent variants
2. Follow-the-regularized-leader
3. Multiplicative weights algorithm
4. UCB and Thompson sampling for bandits
5. Contextual bandit algorithms
6. Regret analysis tools

## Experiments

1. **Regret Comparison**: Different algorithms on same sequence
2. **Adversarial vs Stochastic**: Performance in different settings
3. **Parameter Sensitivity**: Effect of learning rates
4. **Online-to-Batch**: Verify theoretical predictions

## Research Connections

### Foundational Papers
1. Freund & Schapire (1997) - "A Decision-Theoretic Generalization of On-Line Learning"
2. Zinkevich (2003) - "Online Convex Programming and Generalized Infinitesimal Gradient Ascent"
3. Hazan et al. (2007) - "Logarithmic Regret Algorithms for Online Convex Optimization"

### Modern Developments
1. McMahan & Streeter (2010) - "Adaptive Bound Optimization for Online Convex Optimization"
2. Rakhlin et al. (2012) - "Online Learning: Random Averages, Combinatorial Parameters"
3. Foster et al. (2018) - "Contextual Bandits with Surrogate Losses"

## Resources

### Primary Sources
1. **Shalev-Shwartz - Online Learning and Online Convex Optimization**
   - Comprehensive treatment
2. **Cesa-Bianchi & Lugosi - Prediction, Learning, and Games**
   - Game-theoretic perspective
3. **Hazan - Introduction to Online Convex Optimization**
   - Modern algorithmic approach

### Advanced Reading
1. **Lattimore & Szepesv√°ri - Bandit Algorithms**
   - Comprehensive bandit book
2. **Rakhlin et al. - Statistical Learning Theory**
   - Connections to statistical learning
3. **Bubeck & Cesa-Bianchi - Regret Analysis**
   - Survey of regret bounds

## Socratic Questions

### Understanding
1. Why is the adversarial setting more challenging than stochastic?
2. How does regret relate to generalization?
3. What's the role of regularization in online learning?

### Extension
1. Can you design online algorithms for structured prediction?
2. How do we handle non-convex online problems?
3. What happens with delayed or partial feedback?

### Research
1. How can we get adaptive regret bounds?
2. What's the connection between online learning and game theory?
3. How do we design practical online algorithms for deep learning?

## Exercises

### Theoretical
1. Prove the regret bound for online gradient descent
2. Derive the multiplicative weights update rule
3. Show the online-to-batch conversion theorem

### Implementation
1. Implement online learning algorithms from scratch
2. Build bandit simulation environment
3. Create regret visualization tools

### Research
1. Study online learning for neural networks
2. Investigate adaptive learning rate schemes
3. Explore applications to reinforcement learning