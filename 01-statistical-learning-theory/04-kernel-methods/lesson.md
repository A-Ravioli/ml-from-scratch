# Kernel Methods and Reproducing Kernel Hilbert Spaces

## Prerequisites
- Linear algebra (inner products, eigendecompositions)
- Functional analysis (Hilbert spaces, operators)
- Optimization theory (convex optimization, duality)

## Learning Objectives
- Master kernel theory and RKHS fundamentals
- Understand kernel algorithms (SVM, kernel ridge regression)
- Connect kernels to feature maps and function spaces
- Apply kernel methods to modern ML problems

## Mathematical Foundations

### 1. Kernels and Feature Maps

#### Definition 1.1 (Kernel Function)
A function k: ğ’³ Ã— ğ’³ â†’ â„ is a kernel if there exists a feature map Ï†: ğ’³ â†’ â„‹ into a Hilbert space â„‹ such that:
k(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©_â„‹

#### Definition 1.2 (Positive Definite Kernel)
k is positive definite if for all n âˆˆ â„•, xâ‚, ..., xâ‚™ âˆˆ ğ’³, the Gram matrix K with Káµ¢â±¼ = k(xáµ¢, xâ±¼) is positive semidefinite.

#### Theorem 1.1 (Mercer's Theorem)
A continuous kernel k on compact ğ’³ is positive definite âŸº it has an expansion:
k(x, x') = âˆ‘áµ¢â‚Œâ‚^âˆ Î»áµ¢Ï†áµ¢(x)Ï†áµ¢(x')

where Î»áµ¢ â‰¥ 0 and Ï†áµ¢ are eigenfunctions of the integral operator.

### 2. Examples of Kernels

#### Linear Kernel
k(x, x') = x^T x'

**Feature map**: Ï†(x) = x (identity map)
**Properties**: Simplest kernel, equivalent to linear methods

#### Polynomial Kernel
k(x, x') = (x^T x' + c)^d

**Feature map**: All monomials of degree â‰¤ d
**Dimension**: (n+d choose d) for n-dimensional input

#### RBF/Gaussian Kernel
k(x, x') = exp(-||x - x'||Â²/(2ÏƒÂ²))

**Feature map**: Infinite-dimensional
**Properties**: Universal kernel, smooth functions

#### String Kernels
For sequences/text data:
- Subsequence kernels
- Gap-weighted kernels
- Mismatch kernels

### 3. Reproducing Kernel Hilbert Spaces

#### Definition 3.1 (RKHS)
A Hilbert space â„‹ of functions f: ğ’³ â†’ â„ is an RKHS if the evaluation functional Î´â‚“(f) = f(x) is continuous for all x âˆˆ ğ’³.

#### Theorem 3.1 (Riesz Representation)
For RKHS â„‹, âˆƒ!kâ‚“ âˆˆ â„‹ such that f(x) = âŸ¨f, kâ‚“âŸ©_â„‹ for all f âˆˆ â„‹.

#### Definition 3.2 (Reproducing Kernel)
k(x, x') = âŸ¨kâ‚“, kâ‚“'âŸ©_â„‹ = kâ‚“'(x)

#### Reproducing Property
f(x) = âŸ¨f, kâ‚“âŸ©_â„‹ for all f âˆˆ â„‹, x âˆˆ ğ’³

### 4. Moore-Aronszajn Theorem

#### Theorem 4.1 (Moore-Aronszajn)
There is a bijection between positive definite kernels and RKHSs.

**Construction**: Given kernel k, define:
â„‹ = span{kâ‚“ : x âˆˆ ğ’³}

with inner product âŸ¨âˆ‘áµ¢ Î±áµ¢kâ‚“áµ¢, âˆ‘â±¼ Î²â±¼káµ§â±¼âŸ© = âˆ‘áµ¢â±¼ Î±áµ¢Î²â±¼k(xáµ¢, yâ±¼)

**Completion**: Take closure to get Hilbert space.

### 5. Support Vector Machines

#### Primal Problem
minimize (1/2)||w||Â² + Câˆ‘áµ¢ Î¾áµ¢
subject to yáµ¢(w^T Ï†(xáµ¢) + b) â‰¥ 1 - Î¾áµ¢, Î¾áµ¢ â‰¥ 0

#### Dual Problem
maximize âˆ‘áµ¢ Î±áµ¢ - (1/2)âˆ‘áµ¢â±¼ Î±áµ¢Î±â±¼yáµ¢yâ±¼k(xáµ¢, xâ±¼)
subject to 0 â‰¤ Î±áµ¢ â‰¤ C, âˆ‘áµ¢ Î±áµ¢yáµ¢ = 0

#### Representer Theorem
The solution has the form:
w = âˆ‘áµ¢ Î±áµ¢yáµ¢Ï†(xáµ¢)

So f(x) = âˆ‘áµ¢ Î±áµ¢yáµ¢k(xáµ¢, x) + b

#### KKT Conditions
- Î±áµ¢ = 0 âŸ¹ yáµ¢f(xáµ¢) â‰¥ 1 (non-support vectors)
- 0 < Î±áµ¢ < C âŸ¹ yáµ¢f(xáµ¢) = 1 (support vectors on margin)
- Î±áµ¢ = C âŸ¹ yáµ¢f(xáµ¢) â‰¤ 1 (support vectors inside margin)

### 6. Kernel Ridge Regression

#### Problem
minimize âˆ‘áµ¢ (yáµ¢ - f(xáµ¢))Â² + Î»||f||Â²_â„‹

#### Solution
By representer theorem:
f(x) = âˆ‘áµ¢ Î±áµ¢k(xáµ¢, x)

where Î± = (K + Î»I)â»Â¹y and K is the Gram matrix.

#### Connection to Gaussian Processes
Kernel ridge regression is equivalent to GP regression with:
- Prior: f ~ GP(0, k)
- Likelihood: y|f ~ N(f, ÏƒÂ²I)

### 7. Kernel PCA

#### Problem
Find principal components in feature space Ï†(ğ’³).

#### Solution
1. Center kernel matrix: KÌƒ = K - 1â‚™K - K1â‚™ + 1â‚™K1â‚™
2. Solve eigenvalue problem: KÌƒÎ± = Î»Î±
3. Principal components: âˆ‘áµ¢ Î±áµ¢áµÏ†(xáµ¢) where Î±áµ is k-th eigenvector

#### Projections
Project new point x onto k-th component:
âŸ¨Ï†(x), vâ‚–âŸ© = âˆ‘áµ¢ Î±áµ¢áµk(x, xáµ¢)

### 8. Kernel Construction

#### Closure Properties
If kâ‚, kâ‚‚ are kernels:
- kâ‚ + kâ‚‚ is a kernel
- ckâ‚ is a kernel for c â‰¥ 0
- kâ‚kâ‚‚ is a kernel
- exp(kâ‚) is a kernel

#### Tensor Products
For kernels kâ‚ on ğ’³â‚, kâ‚‚ on ğ’³â‚‚:
k((xâ‚, xâ‚‚), (xâ‚', xâ‚‚')) = kâ‚(xâ‚, xâ‚')kâ‚‚(xâ‚‚, xâ‚‚')

#### String Kernels
For sequences s, t:
k(s, t) = âˆ‘_{uâˆˆÎ£*} Ï†áµ¤(s)Ï†áµ¤(t)

where Ï†áµ¤(s) weights occurrences of subsequence u in s.

### 9. Learning Theory for Kernels

#### Rademacher Complexity
For RKHS with radius R:
â„›â‚˜(ğ”¹_R) â‰¤ Râˆš(tr(K)/m)

where ğ”¹_R = {f âˆˆ â„‹ : ||f||_â„‹ â‰¤ R}.

#### Generalization Bound
With probability â‰¥ 1 - Î´:
R(f) â‰¤ RÌ‚(f) + 2Râˆš(tr(K)/m) + âˆš(log(1/Î´)/(2m))

#### Capacity Control
- Small ||f||_â„‹ âŸ¹ simple function âŸ¹ good generalization
- Regularization Î»||f||Â²_â„‹ controls complexity

### 10. Computational Aspects

#### Kernel Matrix Properties
- Size: n Ã— n (expensive for large n)
- Positive semidefinite
- Often dense (no sparsity)

#### Approximation Methods

##### NystrÃ¶m Approximation
Sample m â‰ª n points, approximate:
K â‰ˆ K_{nm}K_{mm}â»Â¹K_{mn}

##### Random Features
For translation-invariant kernels:
k(x, x') = E_Ï‰[Ï†_Ï‰(x)Ï†_Ï‰(x')]

Sample Ï‰â‚, ..., Ï‰â‚˜ and use Ï†(x) = [Ï†_Ï‰â‚(x), ..., Ï†_Ï‰â‚˜(x)].

##### Structured Kernels
- Circulant matrices: Fast via FFT
- Hierarchical methods: Fast multipole
- Toeplitz structure: Leveraged for efficiency

### 11. Multiple Kernel Learning

#### Problem
Learn optimal combination of kernels:
k(x, x') = âˆ‘â±¼ Î²â±¼kâ±¼(x, x')

subject to constraints on Î².

#### Approaches
- Convex combinations: âˆ‘â±¼ Î²â±¼ = 1, Î²â±¼ â‰¥ 0
- â„“â‚š regularization: ||Î²||â‚š â‰¤ 1
- Group regularization: Mixed norms

### 12. Modern Connections

#### Deep Networks as Kernels
In infinite width limit, neural networks correspond to specific kernels:
- Neural Tangent Kernel (NTK)
- Gaussian Process limit

#### Kernel Mean Embeddings
Embed distributions into RKHS:
Î¼â‚š = E_{x~P}[Ï†(x)]

Applications:
- Two-sample testing
- Independence testing
- Causal inference

#### Optimal Transport
Wasserstein distance can be kernelized for efficient computation.

## Implementation Details

See `exercise.py` for implementations of:
1. Basic kernel functions and properties
2. SVM solver using SMO algorithm
3. Kernel ridge regression
4. Kernel PCA
5. Multiple kernel learning
6. Kernel approximation methods

## Experiments

1. **Kernel Comparison**: Different kernels on same dataset
2. **Parameter Sensitivity**: Effect of kernel parameters
3. **Computational Scaling**: Exact vs approximate methods
4. **Feature Visualization**: Kernel PCA projections

## Research Connections

### Foundational Papers
1. Vapnik (1995) - "The Nature of Statistical Learning Theory"
2. SchÃ¶lkopf et al. (1998) - "Nonlinear Component Analysis"
3. Shawe-Taylor & Cristianini (2004) - "Kernel Methods for Pattern Analysis"

### Modern Developments
1. Rahimi & Recht (2007) - "Random Features for Large-Scale Kernel Machines"
2. Bach (2008) - "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning"
3. Jacot et al. (2018) - "Neural Tangent Kernel"

## Resources

### Primary Sources
1. **SchÃ¶lkopf & Smola - Learning with Kernels**
   - Comprehensive kernel methods textbook
2. **Shawe-Taylor & Cristianini - Kernel Methods**
   - Theoretical foundations
3. **Steinwart & Christmann - Support Vector Machines**
   - Rigorous mathematical treatment

### Advanced Reading
1. **Berlinet & Thomas-Agnan - RKHS in Probability**
   - Deep theoretical treatment
2. **Cucker & Smale - Learning Theory**
   - Mathematical foundations
3. **Sriperumbudur et al. - Kernel Embeddings**
   - Modern applications

## Socratic Questions

### Understanding
1. Why do kernels enable nonlinear learning with linear algorithms?
2. What's the connection between kernels and feature maps?
3. How does the reproducing property work?

### Extension
1. Can you design kernels for structured data?
2. How do you choose appropriate kernel parameters?
3. What are the computational tradeoffs of different kernels?

### Research
1. How do modern deep networks relate to kernel methods?
2. Can we design adaptive kernels that learn from data?
3. What's the role of kernels in modern ML theory?

## Exercises

### Theoretical
1. Prove that the Gaussian kernel is positive definite
2. Derive the dual SVM formulation from the primal
3. Show that kernel ridge regression minimizes regularized risk

### Implementation
1. Implement SVM from scratch using SMO
2. Build kernel PCA for dimensionality reduction
3. Create multiple kernel learning algorithm

### Research
1. Study neural tangent kernels empirically
2. Investigate kernel approximation methods
3. Explore applications to structured prediction