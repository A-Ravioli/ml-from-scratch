"""
Multi-Head Attention Implementation Exercise

Implement multi-head self-attention from scratch, focusing on parallel computation
and head specialization analysis.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, Tuple, List, Optional, Dict, Union
from abc import ABC, abstractmethod
import time


class MultiHeadAttention:
    """
    Multi-head self-attention mechanism
    
    Implements parallel attention heads with efficient computation
    """
    
    def __init__(self, d_model: int, num_heads: int, dropout_rate: float = 0.1):
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.d_v = d_model // num_heads
        self.dropout_rate = dropout_rate
        
        # Combined projection matrices for efficiency
        self.W_qkv = np.random.randn(d_model, 3 * d_model) * np.sqrt(2.0 / d_model)
        
        # Output projection
        self.W_o = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        
        # Store attention weights for analysis
        self.last_attention_weights = None
        self.head_attention_weights = None
    
    def scaled_dot_product_attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                                   mask: Optional[np.ndarray] = None,
                                   dropout_rate: float = 0.0) -> Tuple[np.ndarray, np.ndarray]:
        """
        Scaled dot-product attention for a single head or batch of heads
        
        Args:
            Q: Queries [batch_size, (num_heads), seq_len, d_k]
            K: Keys [batch_size, (num_heads), seq_len, d_k] 
            V: Values [batch_size, (num_heads), seq_len, d_v]
            mask: Optional attention mask
            dropout_rate: Dropout rate for attention weights
            
        Returns:
            attention_output: [batch_size, (num_heads), seq_len, d_v]
            attention_weights: [batch_size, (num_heads), seq_len, seq_len]
        """
        # TODO: Implement efficient scaled dot-product attention
        # 1. Compute attention scores
        # 2. Apply scaling
        # 3. Apply mask if provided
        # 4. Apply softmax
        # 5. Apply dropout if specified
        # 6. Multiply by values
        
        d_k = Q.shape[-1]
        
        # Step 1 & 2: Compute scaled attention scores
        scores = np.matmul(Q, K.swapaxes(-2, -1)) / np.sqrt(d_k)
        
        # Step 3: Apply mask if provided
        if mask is not None:
            scores = scores + (mask * -1e9)
        
        # Step 4: Apply softmax
        scores_max = np.max(scores, axis=-1, keepdims=True)
        exp_scores = np.exp(scores - scores_max)
        attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
        
        # Step 5: Apply dropout (simplified - just for demonstration)
        if dropout_rate > 0.0:
            dropout_mask = np.random.random(attention_weights.shape) > dropout_rate
            attention_weights = attention_weights * dropout_mask / (1 - dropout_rate)
        
        # Step 6: Apply to values
        attention_output = np.matmul(attention_weights, V)
        
        return attention_output, attention_weights
    
    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Forward pass through multi-head attention
        
        Args:
            X: Input sequences [batch_size, seq_len, d_model]
            mask: Optional attention mask [batch_size, seq_len, seq_len]
            
        Returns:
            output: Transformed sequences [batch_size, seq_len, d_model]
        """
        # TODO: Implement multi-head attention forward pass
        # 1. Project to Q, K, V for all heads simultaneously
        # 2. Reshape for multi-head computation
        # 3. Apply attention for all heads in parallel
        # 4. Concatenate heads
        # 5. Apply output projection
        
        batch_size, seq_len, d_model = X.shape
        
        # Step 1: Project to Q, K, V for all heads
        qkv = np.matmul(X, self.W_qkv)  # [batch_size, seq_len, 3*d_model]
        
        # Split into Q, K, V
        Q, K, V = np.split(qkv, 3, axis=-1)  # Each: [batch_size, seq_len, d_model]
        
        # Step 2: Reshape for multi-head computation
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_v).transpose(0, 2, 1, 3)
        # Now: [batch_size, num_heads, seq_len, d_k/d_v]
        
        # Step 3: Apply attention for all heads
        if mask is not None:
            # Expand mask for all heads
            mask = mask[:, None, :, :]  # [batch_size, 1, seq_len, seq_len]
        
        attention_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask, self.dropout_rate
        )
        # attention_output: [batch_size, num_heads, seq_len, d_v]
        # attention_weights: [batch_size, num_heads, seq_len, seq_len]
        
        # Store attention weights for analysis
        self.head_attention_weights = attention_weights
        self.last_attention_weights = np.mean(attention_weights, axis=1)  # Average across heads
        
        # Step 4: Concatenate heads
        attention_output = attention_output.transpose(0, 2, 1, 3)  # [batch_size, seq_len, num_heads, d_v]
        attention_output = attention_output.reshape(batch_size, seq_len, self.d_model)
        
        # Step 5: Apply output projection
        output = np.matmul(attention_output, self.W_o)
        
        return output
    
    def get_attention_weights(self, head_idx: Optional[int] = None) -> np.ndarray:
        """
        Get attention weights from specific head or averaged across heads
        
        Args:
            head_idx: Specific head index, or None for average
            
        Returns:
            attention_weights: Attention weights
        """
        if head_idx is not None:
            return self.head_attention_weights[:, head_idx, :, :]
        else:
            return self.last_attention_weights
    
    def get_all_head_weights(self) -> Optional[np.ndarray]:
        """Get attention weights from all heads"""
        return self.head_attention_weights
    
    def count_parameters(self) -> int:
        """Count total number of parameters"""
        return self.W_qkv.size + self.W_o.size


class MultiQueryAttention:
    """
    Multi-Query Attention (MQA) - shared keys and values, separate queries
    
    More memory efficient for inference while maintaining much of the expressivity
    """
    
    def __init__(self, d_model: int, num_heads: int):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.d_v = d_model // num_heads
        
        # Separate query projections for each head
        self.W_q = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        
        # Shared key and value projections
        self.W_k = np.random.randn(d_model, self.d_k) * np.sqrt(2.0 / d_model)
        self.W_v = np.random.randn(d_model, self.d_v) * np.sqrt(2.0 / d_model)
        
        # Output projection
        self.W_o = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        
        self.last_attention_weights = None
    
    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """Forward pass through multi-query attention"""
        # TODO: Implement multi-query attention
        # 1. Project to separate Q for each head, shared K, V
        # 2. Apply attention for each head with shared K, V
        # 3. Concatenate and project output
        
        batch_size, seq_len, d_model = X.shape
        
        # Step 1: Project to Q (all heads), shared K, V
        Q_all = np.matmul(X, self.W_q)  # [batch_size, seq_len, d_model]
        K = np.matmul(X, self.W_k)      # [batch_size, seq_len, d_k]
        V = np.matmul(X, self.W_v)      # [batch_size, seq_len, d_v]
        
        # Reshape Q for multi-head
        Q = Q_all.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        # Q: [batch_size, num_heads, seq_len, d_k]
        # K, V: [batch_size, seq_len, d_k/d_v] (shared across heads)
        
        # Step 2: Apply attention for each head with shared K, V
        head_outputs = []
        attention_weights_list = []
        
        for i in range(self.num_heads):
            Q_i = Q[:, i, :, :]  # [batch_size, seq_len, d_k]
            
            # Compute attention scores
            scores = np.matmul(Q_i, K.transpose(0, 2, 1)) / np.sqrt(self.d_k)
            
            if mask is not None:
                scores = scores + (mask * -1e9)
            
            # Softmax
            scores_max = np.max(scores, axis=-1, keepdims=True)
            exp_scores = np.exp(scores - scores_max)
            attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
            
            # Apply to values
            head_output = np.matmul(attn_weights, V)  # [batch_size, seq_len, d_v]
            
            head_outputs.append(head_output)
            attention_weights_list.append(attn_weights)
        
        # Step 3: Concatenate heads
        concatenated = np.concatenate(head_outputs, axis=-1)  # [batch_size, seq_len, d_model]
        
        # Output projection
        output = np.matmul(concatenated, self.W_o)
        
        # Store attention weights (average across heads)
        self.last_attention_weights = np.mean(np.stack(attention_weights_list, axis=1), axis=1)
        
        return output
    
    def count_parameters(self) -> int:
        """Count parameters - should be fewer than standard multi-head"""
        return self.W_q.size + self.W_k.size + self.W_v.size + self.W_o.size


class GroupedQueryAttention:
    """
    Grouped-Query Attention (GQA) - intermediate between MHA and MQA
    
    Divides heads into groups, shares K,V within groups
    """
    
    def __init__(self, d_model: int, num_heads: int, num_groups: int):
        assert num_heads % num_groups == 0, "num_heads must be divisible by num_groups"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_groups = num_groups
        self.heads_per_group = num_heads // num_groups
        self.d_k = d_model // num_heads
        self.d_v = d_model // num_heads
        
        # Query projections (separate for each head)
        self.W_q = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        
        # Key and value projections (shared within groups)
        self.W_k = np.random.randn(d_model, num_groups * self.d_k) * np.sqrt(2.0 / d_model)
        self.W_v = np.random.randn(d_model, num_groups * self.d_v) * np.sqrt(2.0 / d_model)
        
        # Output projection
        self.W_o = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
    
    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """Forward pass through grouped-query attention"""
        # TODO: Implement grouped-query attention
        # This is a more complex variant - simplified implementation
        
        # For now, delegate to standard multi-head attention
        # In practice, would implement efficient grouped computation
        mha = MultiHeadAttention(self.d_model, self.num_heads)
        return mha.forward(X, mask)


def analyze_head_specialization(attention_weights: np.ndarray, 
                               tokens: Optional[List[str]] = None) -> Dict:
    """
    Analyze specialization patterns across attention heads
    
    Args:
        attention_weights: [batch_size, num_heads, seq_len, seq_len]
        tokens: Optional token strings
        
    Returns:
        analysis: Dictionary with head specialization metrics
    """
    batch_size, num_heads, seq_len, _ = attention_weights.shape
    
    analysis = {
        'head_entropy': [],
        'head_diversity': [],
        'positional_preferences': [],
        'head_similarity_matrix': None,
        'specialized_patterns': {}
    }
    
    # TODO: Implement head specialization analysis
    # 1. Compute attention entropy for each head
    # 2. Measure head diversity/similarity  
    # 3. Identify positional preferences
    # 4. Detect common attention patterns
    
    # Average across batch for analysis
    avg_attention = np.mean(attention_weights, axis=0)  # [num_heads, seq_len, seq_len]
    
    # Step 1: Head entropy
    for h in range(num_heads):
        head_weights = avg_attention[h]  # [seq_len, seq_len]
        
        entropy_per_position = []
        for i in range(seq_len):
            pos_weights = head_weights[i]
            entropy = -np.sum(pos_weights * np.log(pos_weights + 1e-8))
            entropy_per_position.append(entropy)
        
        analysis['head_entropy'].append(np.mean(entropy_per_position))
    
    # Step 2: Head similarity matrix
    head_similarity = np.zeros((num_heads, num_heads))
    for i in range(num_heads):
        for j in range(num_heads):
            # Compute correlation between attention patterns
            attn_i = avg_attention[i].flatten()
            attn_j = avg_attention[j].flatten()
            
            correlation = np.corrcoef(attn_i, attn_j)[0, 1]
            head_similarity[i, j] = correlation if not np.isnan(correlation) else 0.0
    
    analysis['head_similarity_matrix'] = head_similarity
    
    # Step 3: Positional preferences
    for h in range(num_heads):
        head_weights = avg_attention[h]
        
        # Analyze diagonal attention (self-attention)
        diagonal_strength = np.mean(np.diag(head_weights))
        
        # Analyze local vs global attention
        local_mask = np.abs(np.arange(seq_len)[:, None] - np.arange(seq_len)[None, :]) <= 2
        local_attention = np.mean(head_weights[local_mask])
        
        analysis['positional_preferences'].append({
            'diagonal_strength': diagonal_strength,
            'local_attention_ratio': local_attention,
            'max_attention_distance': np.unravel_index(np.argmax(head_weights), head_weights.shape)
        })
    
    # Step 4: Detect specialized patterns
    # Identify heads with specific patterns
    for h in range(num_heads):
        head_weights = avg_attention[h]
        
        # Check for diagonal pattern (self-attention)
        diagonal_score = np.mean(np.diag(head_weights)) / np.mean(head_weights)
        
        # Check for adjacent token pattern
        adjacent_score = 0
        for i in range(seq_len - 1):
            adjacent_score += head_weights[i, i + 1] + head_weights[i + 1, i]
        adjacent_score = adjacent_score / (2 * (seq_len - 1)) / np.mean(head_weights)
        
        pattern_type = "general"
        if diagonal_score > 2.0:
            pattern_type = "self_attention"
        elif adjacent_score > 2.0:
            pattern_type = "adjacent_tokens"
        elif analysis['head_entropy'][h] > np.log(seq_len) * 0.8:
            pattern_type = "global_uniform"
        
        analysis['specialized_patterns'][f'head_{h}'] = {
            'pattern_type': pattern_type,
            'diagonal_score': diagonal_score,
            'adjacent_score': adjacent_score
        }
    
    return analysis


def compute_head_importance(model: MultiHeadAttention, 
                          X: np.ndarray, 
                          target_metric: Callable = None) -> np.ndarray:
    """
    Compute importance of each attention head
    
    Args:
        model: Multi-head attention model
        X: Input data [batch_size, seq_len, d_model]
        target_metric: Function to compute performance metric
        
    Returns:
        importance_scores: Importance score for each head [num_heads]
    """
    if target_metric is None:
        # Use attention output norm as proxy metric
        target_metric = lambda output: np.mean(np.linalg.norm(output, axis=-1))
    
    # Baseline performance
    baseline_output = model.forward(X)
    baseline_score = target_metric(baseline_output)
    
    importance_scores = np.zeros(model.num_heads)
    
    # TODO: Implement head importance computation
    # 1. For each head, compute performance without that head
    # 2. Importance = baseline - performance_without_head
    
    for head_idx in range(model.num_heads):
        # Simulate removing head by zeroing its contribution
        # This is a simplified approach - full implementation would require
        # proper head masking during forward pass
        
        # For demonstration, use a heuristic based on attention weight variance
        all_head_weights = model.get_all_head_weights()
        if all_head_weights is not None:
            head_weights = all_head_weights[:, head_idx, :, :]  # [batch_size, seq_len, seq_len]
            
            # Use attention weight variance as importance proxy
            importance_scores[head_idx] = np.var(head_weights)
        else:
            importance_scores[head_idx] = 1.0 / model.num_heads  # Equal importance if no data
    
    return importance_scores


def prune_attention_heads(model: MultiHeadAttention, 
                         importance_scores: np.ndarray,
                         keep_ratio: float = 0.75) -> MultiHeadAttention:
    """
    Prune less important attention heads
    
    Args:
        model: Original multi-head attention model
        importance_scores: Importance score for each head
        keep_ratio: Fraction of heads to keep
        
    Returns:
        pruned_model: New model with fewer heads
    """
    num_heads_to_keep = int(model.num_heads * keep_ratio)
    
    # Select top-k most important heads
    top_head_indices = np.argsort(importance_scores)[-num_heads_to_keep:]
    
    # TODO: Create new model with only selected heads
    # For simplicity, create a new model with reduced head count
    
    pruned_model = MultiHeadAttention(model.d_model, num_heads_to_keep)
    
    # In practice, would copy weights from selected heads
    # This is a simplified implementation
    
    return pruned_model


def visualize_attention_heads(attention_weights: np.ndarray,
                            tokens: Optional[List[str]] = None,
                            max_heads_display: int = 8):
    """
    Visualize attention patterns from multiple heads
    
    Args:
        attention_weights: [num_heads, seq_len, seq_len]
        tokens: Optional token strings
        max_heads_display: Maximum number of heads to display
    """
    num_heads = min(attention_weights.shape[0], max_heads_display)
    
    cols = min(4, num_heads)
    rows = (num_heads + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))
    
    if num_heads == 1:
        axes = [axes]
    elif rows == 1:
        axes = [axes] if cols == 1 else list(axes)
    else:
        axes = axes.flatten()
    
    for i in range(num_heads):
        ax = axes[i]
        
        # Plot attention heatmap
        im = ax.imshow(attention_weights[i], cmap='Blues', aspect='auto')
        ax.set_title(f'Head {i + 1}')
        
        # Add token labels if provided and sequence is short
        if tokens is not None and len(tokens) <= 20:
            ax.set_xticks(range(len(tokens)))
            ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
            ax.set_yticks(range(len(tokens)))
            ax.set_yticklabels(tokens, fontsize=8)
        
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')
        
        # Add colorbar
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    # Hide unused subplots
    for i in range(num_heads, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.show()


def compare_attention_variants(seq_len: int, d_model: int, num_heads: int) -> Dict:
    """
    Compare different attention variants
    
    Args:
        seq_len: Sequence length
        d_model: Model dimension
        num_heads: Number of attention heads
        
    Returns:
        comparison: Performance and efficiency comparison
    """
    batch_size = 2
    X = np.random.randn(batch_size, seq_len, d_model)
    
    variants = {
        'Multi-Head Attention': MultiHeadAttention(d_model, num_heads),
        'Multi-Query Attention': MultiQueryAttention(d_model, num_heads),
    }
    
    comparison = {}
    
    for name, model in variants.items():
        # Time forward pass
        start_time = time.time()
        for _ in range(10):  # Multiple runs for better timing
            output = model.forward(X)
        end_time = time.time()
        avg_time = (end_time - start_time) / 10
        
        comparison[name] = {
            'forward_time': avg_time,
            'parameters': model.count_parameters(),
            'output_shape': output.shape,
            'memory_efficiency': 'High' if 'Query' in name else 'Standard'
        }
    
    return comparison


# ============================================================================
# EXERCISES
# ============================================================================

def exercise_1_multi_head_implementation():
    """
    Exercise 1: Implement multi-head attention
    
    Tasks:
    1. Complete MultiHeadAttention implementation
    2. Test parallel head computation
    3. Verify output shapes and properties
    4. Compare with single-head attention
    """
    
    print("=== Exercise 1: Multi-Head Attention Implementation ===")
    
    # TODO: Test multi-head attention implementation
    
    # Create test data
    batch_size, seq_len, d_model = 2, 8, 64
    num_heads = 8
    
    X = np.random.randn(batch_size, seq_len, d_model)
    mha = MultiHeadAttention(d_model, num_heads)
    
    # Forward pass
    output = mha.forward(X)
    
    print(f"Input shape: {X.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Number of heads: {num_heads}")
    print(f"d_k per head: {mha.d_k}")
    print(f"Parameters: {mha.count_parameters():,}")
    
    # Test attention weights
    all_head_weights = mha.get_all_head_weights()
    if all_head_weights is not None:
        print(f"Attention weights shape: {all_head_weights.shape}")
        
        # Verify attention properties for each head
        for h in range(num_heads):
            head_weights = all_head_weights[0, h, :, :]  # First batch item
            row_sums = np.sum(head_weights, axis=1)
            print(f"Head {h+1} attention sum: {np.mean(row_sums):.4f}")
    
    assert output.shape == X.shape, "Output should match input shape"
    
    pass


def exercise_2_head_specialization():
    """
    Exercise 2: Analyze attention head specialization
    
    Tasks:
    1. Implement head specialization analysis
    2. Visualize different head patterns
    3. Measure head diversity
    4. Identify specialized head types
    """
    
    print("=== Exercise 2: Head Specialization Analysis ===")
    
    # TODO: Analyze head specialization patterns
    
    pass


def exercise_3_attention_variants():
    """
    Exercise 3: Implement attention variants
    
    Tasks:
    1. Complete MultiQueryAttention implementation
    2. Implement GroupedQueryAttention
    3. Compare efficiency and performance
    4. Analyze trade-offs
    """
    
    print("=== Exercise 3: Attention Variants ===")
    
    # TODO: Test different attention variants
    
    pass


def exercise_4_head_pruning():
    """
    Exercise 4: Implement head pruning
    
    Tasks:
    1. Implement head importance scoring
    2. Create head pruning algorithm
    3. Analyze pruning effects on performance
    4. Find minimal head configurations
    """
    
    print("=== Exercise 4: Head Pruning ===")
    
    # TODO: Implement head pruning techniques
    
    pass


def exercise_5_efficient_computation():
    """
    Exercise 5: Optimize multi-head computation
    
    Tasks:
    1. Implement batched head computation
    2. Optimize memory usage
    3. Compare computational efficiency
    4. Implement gradient checkpointing
    """
    
    print("=== Exercise 5: Efficient Computation ===")
    
    # TODO: Optimize multi-head attention computation
    
    pass


def exercise_6_interpretability_analysis():
    """
    Exercise 6: Multi-head attention interpretability
    
    Tasks:
    1. Analyze head attention patterns
    2. Study head evolution during training
    3. Probe head functionality
    4. Compare interpretability across tasks
    """
    
    print("=== Exercise 6: Interpretability Analysis ===")
    
    # TODO: Comprehensive interpretability analysis
    
    pass


if __name__ == "__main__":
    # Run all exercises
    exercise_1_multi_head_implementation()
    exercise_2_head_specialization()
    exercise_3_attention_variants()
    exercise_4_head_pruning()
    exercise_5_efficient_computation()
    exercise_6_interpretability_analysis()
    
    print("\nAll exercises completed!")
    print("Key insights to understand:")
    print("1. Multi-head parallel computation and efficiency")
    print("2. Head specialization and diversity patterns")
    print("3. Attention variants and their trade-offs")
    print("4. Head pruning and model compression techniques")