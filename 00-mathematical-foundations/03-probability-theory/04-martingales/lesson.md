# Martingales for Machine Learning

## Prerequisites

- Measure-theoretic probability, filtrations, conditional expectation

## Learning Objectives

- Understand martingales, super/submartingales, and stopping times
- Apply optional stopping and Azumaâ€“Hoeffding inequality
- Use martingale tools in online learning and bandits

## Core Concepts

### 1. Filtrations and Martingales

Adapted processes (X_t) with E[|X_t|] < âˆž and E[X_{t+1} | ð”½_t] = X_t.

#### Intuition

- â€œFair gameâ€: best prediction at next step is current value.

### 2. Optional Stopping

Under mild conditions, E[X_Ï„] = E[X_0] for a stopping time Ï„.

### 3. Azumaâ€“Hoeffding Inequality

For martingales with bounded increments: P(X_n âˆ’ X_0 â‰¥ t) â‰¤ exp(âˆ’2tÂ² / âˆ‘c_iÂ²).

## ML Connections

- Regret bounds in online learning; concentration for dependent sequences.

## Implementation Details

See `exercise.py` for:

1. `is_martingale` via conditional expectation checks (empirical)
2. `optional_stopping_demo`
3. `azuma_hoeffding_bound` and empirical verification


