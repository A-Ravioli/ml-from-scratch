# Denoising Diffusion Probabilistic Models: State-of-the-Art Generation

## Prerequisites
- Stochastic processes and Markov chains
- Variational inference and ELBO
- Score-based models and Langevin dynamics
- Neural network architectures (U-Net, attention)
- Advanced probability theory (SDEs, Fokker-Planck equations)

## Learning Objectives
- Master the mathematical foundations of diffusion processes for generation
- Understand forward and reverse diffusion processes
- Implement DDPM from scratch with proper training and sampling
- Connect diffusion models to score-based generative models
- Analyze the theoretical properties and state-of-the-art performance

## Mathematical Foundations

### 1. The Diffusion Process Framework

#### Forward Diffusion Process
**Definition**: A Markov chain that gradually adds noise to data

q(xâ‚:T | xâ‚€) = âˆáµ—â‚Œâ‚áµ€ q(xâ‚œ | xâ‚œâ‚‹â‚)

where q(xâ‚œ | xâ‚œâ‚‹â‚) = ð’©(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)

**Properties**:
- Î²â‚œ âˆˆ (0,1): Noise schedule
- xâ‚€: Original data
- xâ‚œ: Noisy version at step T
- As T â†’ âˆž: xâ‚œ â†’ ð’©(0,I)

#### Closed-Form Forward Process
**Key insight**: Can sample xâ‚œ directly from xâ‚€

Define Î±â‚œ = 1 - Î²â‚œ, á¾±â‚œ = âˆâ‚›â‚Œâ‚áµ— Î±â‚›

**Theorem 1.1**: q(xâ‚œ | xâ‚€) = ð’©(xâ‚œ; âˆšá¾±â‚œ xâ‚€, (1-á¾±â‚œ)I)

**Proof**: By induction on the Markov property and Gaussian closure. â–¡

**Reparameterization**: xâ‚œ = âˆšá¾±â‚œ xâ‚€ + âˆš(1-á¾±â‚œ) Îµ, where Îµ ~ ð’©(0,I)

### 2. Reverse Diffusion Process

#### The Reverse Process
**Goal**: Learn to reverse the forward process

p_Î¸(xâ‚€:T) = p(xT) âˆáµ—â‚Œâ‚áµ€ p_Î¸(xâ‚œâ‚‹â‚ | xâ‚œ)

where p_Î¸(xâ‚œâ‚‹â‚ | xâ‚œ) = ð’©(xâ‚œâ‚‹â‚; Î¼_Î¸(xâ‚œ,t), Î£_Î¸(xâ‚œ,t))

#### Tractable Reverse Process
**Theorem 2.1**: For small Î²â‚œ, the reverse process is approximately Gaussian:

q(xâ‚œâ‚‹â‚ | xâ‚œ, xâ‚€) = ð’©(xâ‚œâ‚‹â‚; Î¼Ìƒâ‚œ(xâ‚œ, xâ‚€), Î²Ìƒâ‚œI)

where:
Î¼Ìƒâ‚œ(xâ‚œ, xâ‚€) = (âˆšá¾±â‚œâ‚‹â‚ Î²â‚œ)/(1-á¾±â‚œ) xâ‚€ + (âˆšÎ±â‚œ (1-á¾±â‚œâ‚‹â‚))/(1-á¾±â‚œ) xâ‚œ

Î²Ìƒâ‚œ = (1-á¾±â‚œâ‚‹â‚)/(1-á¾±â‚œ) Î²â‚œ

### 3. Training Objective Derivation

#### Variational Lower Bound
**Goal**: Maximize log p_Î¸(xâ‚€)

log p_Î¸(xâ‚€) â‰¥ ð”¼_q[log p_Î¸(xâ‚€:T)/q(xâ‚:T|xâ‚€)]

**ELBO decomposition**:
â„’ = ð”¼_q[-log p_Î¸(xT) + âˆ‘áµ—â‚Œâ‚‚áµ€ KL(q(xâ‚œâ‚‹â‚|xâ‚œ,xâ‚€) || p_Î¸(xâ‚œâ‚‹â‚|xâ‚œ)) + log p_Î¸(xâ‚€|xâ‚)]

#### Simplified Objective
**Key insight**: KL divergence between Gaussians has closed form

**Theorem 3.1**: The training objective can be written as:
â„’_simple = ð”¼_{t,xâ‚€,Îµ}[||Îµ - Îµ_Î¸(âˆšá¾±â‚œ xâ‚€ + âˆš(1-á¾±â‚œ) Îµ, t)||Â²]

where Îµ_Î¸ predicts the noise added to xâ‚€.

**Significance**: Train a neural network to predict noise!

### 4. DDPM Algorithm

#### Algorithm 4.1 (DDPM Training)
```
repeat:
    xâ‚€ ~ q(xâ‚€)           # Sample data
    t ~ Uniform(1, T)     # Sample timestep
    Îµ ~ N(0, I)          # Sample noise
    
    # Forward process (reparameterization)
    xâ‚œ = âˆšá¾±â‚œ xâ‚€ + âˆš(1-á¾±â‚œ) Îµ
    
    # Predict noise
    Îµ_pred = Îµ_Î¸(xâ‚œ, t)
    
    # Compute loss
    loss = ||Îµ - Îµ_pred||Â²
    
    # Update parameters
    Î¸ â† Î¸ - âˆ‡_Î¸ loss
```

#### Algorithm 4.2 (DDPM Sampling)
```
xâ‚œ ~ N(0, I)             # Start from noise

for t = T, T-1, ..., 1:
    z ~ N(0, I) if t > 1 else 0
    
    # Predict noise
    Îµ_pred = Îµ_Î¸(xâ‚œ, t)
    
    # Compute mean
    Î¼ = (1/âˆšÎ±â‚œ)(xâ‚œ - Î²â‚œ/âˆš(1-á¾±â‚œ) Îµ_pred)
    
    # Sample previous step
    xâ‚œâ‚‹â‚ = Î¼ + âˆšÎ²Ìƒâ‚œ z

return xâ‚€
```

### 5. Connection to Score-Based Models

#### Score Function
**Definition**: âˆ‡_x log p(x)

**Connection**: Noise prediction is related to score:
Îµ_Î¸(xâ‚œ, t) = -âˆš(1-á¾±â‚œ) s_Î¸(xâ‚œ, t)

where s_Î¸ is the score function.

#### Langevin Dynamics
**Classical**: Sample from p(x) using âˆ‡_x log p(x)

x â† x + Î·âˆ‡_x log p(x) + âˆš(2Î·) z

where z ~ ð’©(0,I).

#### Theorem 5.1 (Song et al.)
DDPM sampling is a discretization of reverse-time SDE:
dx = [f(x,t) - gÂ²(t)âˆ‡_x log p_t(x)]dt + g(t)dwÌ„

### 6. Noise Schedules

#### Linear Schedule
Î²â‚œ = Î²â‚ + (t-1)/(T-1)(Î²_T - Î²â‚)

**Parameters**: Î²â‚ = 10â»â´, Î²_T = 0.02, T = 1000

#### Cosine Schedule  
á¾±â‚œ = cosÂ²(Ï€(t/T + s)/(1 + s))

where s = 0.008 (small offset)

**Benefits**: More noise at beginning, less at end

#### Learned Schedules
**Idea**: Learn Î²â‚œ as part of training
**Challenge**: Optimization can be unstable
**Solutions**: Constrain to valid ranges

### 7. Architecture Design

#### U-Net Backbone
**Why U-Net**: 
- Skip connections preserve high-frequency details
- Multi-scale processing
- Proven effectiveness for image-to-image tasks

**Modifications for DDPM**:
- Time embedding: Sinusoidal + MLP
- Group normalization instead of batch norm
- Self-attention at multiple resolutions

#### Time Embedding
**Sinusoidal encoding**:
PE(t, 2i) = sin(t/10000^(2i/d))
PE(t, 2i+1) = cos(t/10000^(2i/d))

**Integration**: Add to each layer via shift and scale

#### Attention Mechanisms
**Self-attention**: At 16Ã—16 and 8Ã—8 resolutions
**Benefit**: Capture long-range dependencies
**Cost**: Quadratic in spatial dimensions

### 8. Advanced Training Techniques

#### Parameterization Variants
**Îµ-parameterization**: Predict noise (standard)
**xâ‚€-parameterization**: Predict original image
**v-parameterization**: Predict velocity field

**v-parameterization**:
v = Î±â‚œ Îµ - Ïƒâ‚œ xâ‚€

where Î±â‚œ = âˆšá¾±â‚œ, Ïƒâ‚œ = âˆš(1-á¾±â‚œ)

#### Improved Training
**Importance sampling**: Weight loss by t
â„’_weighted = ð”¼[w(t) ||Îµ - Îµ_Î¸(xâ‚œ, t)||Â²]

**EMA**: Exponential moving average of parameters
Î¸_ema â† Î¼ Î¸_ema + (1-Î¼) Î¸

### 9. Sampling Improvements

#### DDIM (Denoising Diffusion Implicit Models)
**Key insight**: Use deterministic sampling process

xâ‚œâ‚‹â‚ = âˆšá¾±â‚œâ‚‹â‚ (xâ‚œ - âˆš(1-á¾±â‚œ) Îµ_Î¸(xâ‚œ,t))/âˆšá¾±â‚œ + âˆš(1-á¾±â‚œâ‚‹â‚ - Ïƒâ‚œÂ²) Îµ_Î¸(xâ‚œ,t) + Ïƒâ‚œ Îµ

**Benefits**: 
- Faster sampling (fewer steps)
- Interpolation in latent space
- Deterministic process

#### Classifier Guidance
**Idea**: Use pretrained classifier to guide generation

ÎµÌƒ = Îµ_Î¸(xâ‚œ,t) - âˆš(1-á¾±â‚œ) âˆ‡_xâ‚œ log p_Ï†(y|xâ‚œ)

**Benefits**: Better sample quality and controllability
**Cost**: Requires classifier for each class

#### Classifier-Free Guidance
**Innovation**: Avoid need for separate classifier

ÎµÌƒ = Îµ_Î¸(xâ‚œ,t,âˆ…) + w(Îµ_Î¸(xâ‚œ,t,c) - Îµ_Î¸(xâ‚œ,t,âˆ…))

where w > 1 is guidance scale, c is condition.

### 10. Theoretical Analysis

#### Sample Quality vs Likelihood
**Observation**: DDPMs generate high-quality samples but have poor likelihood

**Explanation**: 
- Likelihood includes all frequencies
- Human perception focuses on low frequencies
- DDPMs excel at perceptually important features

#### Convergence Analysis
**Theorem 10.1**: Under Lipschitz conditions, DDPM sampling converges to true distribution as T â†’ âˆž and network capacity â†’ âˆž.

#### Mode Coverage
**Empirical**: DDPMs show excellent mode coverage
**Theory**: Langevin dynamics is ergodic under regularity conditions

### 11. Extensions and Variants

#### Latent Diffusion Models
**Motivation**: Reduce computational cost
**Approach**: Apply diffusion in latent space of autoencoder

**Architecture**:
1. Train VAE: x â†” z
2. Train diffusion on z
3. Sample: z ~ DDPM, x = decoder(z)

#### Cascaded Diffusion
**Strategy**: Low-res â†’ High-res pipeline
**Implementation**: 
1. Generate 64Ã—64 image
2. Super-resolve to 256Ã—256
3. Super-resolve to 1024Ã—1024

#### Text-to-Image Diffusion
**DALL-E 2**: CLIP embeddings + diffusion
**Imagen**: T5 text encoder + cascaded diffusion
**Stable Diffusion**: Latent diffusion + CLIP

### 12. Computational Considerations

#### Training Efficiency
**Memory**: Store intermediate activations for gradient checkpointing
**Computation**: O(T) forward passes per training step
**Parallelization**: Batch over t dimension

#### Sampling Efficiency
**Standard**: T=1000 sampling steps
**DDIM**: 50-200 steps with similar quality
**Progressive distillation**: Train few-step models
**Consistency models**: One-step generation

#### Hardware Optimization
**Mixed precision**: FP16 for memory and speed
**Gradient checkpointing**: Trade compute for memory
**Compile optimization**: XLA, TorchScript

### 13. Evaluation and Metrics

#### Sample Quality
**FID**: FrÃ©chet Inception Distance
**IS**: Inception Score  
**Precision/Recall**: Mode coverage vs quality
**CLIP Score**: For text-to-image models

#### Diversity Metrics
**LPIPS**: Learned perceptual distance
**MS-SSIM**: Multi-scale structural similarity
**Intra-class diversity**: Within-class variation

#### Human Evaluation
**Gold standard**: Human preference studies
**Aspects**: Realism, diversity, text alignment
**Challenges**: Expensive, subjective

### 14. Applications and Impact

#### Image Generation
**High-resolution**: Up to 1024Ã—1024 and beyond
**Conditional**: Class-conditional, text-to-image
**Style transfer**: Zero-shot style manipulation

#### Text-to-Image
**DALL-E 2**: OpenAI's system
**Imagen**: Google's text-to-image
**Stable Diffusion**: Open-source alternative
**Midjourney**: Commercial artistic tool

#### Other Domains
**Audio**: WaveGrad for audio synthesis
**Video**: Video diffusion models
**3D**: Point cloud and mesh generation
**Molecular**: Drug discovery applications

#### Scientific Applications
**Medical imaging**: MRI, CT scan synthesis
**Climate modeling**: Weather pattern generation
**Astronomy**: Galaxy and star formation simulation

### 15. Societal Implications

#### Positive Applications
**Creative tools**: Artists and designers
**Data augmentation**: Training data synthesis
**Accessibility**: Image generation for visually impaired
**Education**: Visual learning aids

#### Challenges and Concerns
**Deepfakes**: Realistic fake content
**Copyright**: Training on copyrighted images
**Bias**: Reflecting training data biases
**Computational cost**: Environmental impact

#### Ethical Considerations
**Content filtering**: Preventing harmful generation
**Attribution**: Credit for artistic styles
**Regulation**: Governance frameworks needed
**Digital watermarking**: Identifying synthetic content

## Implementation Details

See `exercise.py` for implementations of:
1. Forward and reverse diffusion processes
2. DDPM training loop with proper loss computation
3. U-Net architecture with time embeddings
4. Various noise schedules (linear, cosine)
5. DDIM sampling for faster generation
6. Evaluation metrics and visualization tools

## Experiments

1. **Noise Schedule Comparison**: Linear vs cosine vs learned schedules
2. **Architecture Ablation**: Effect of attention, skip connections, time embedding
3. **Sampling Methods**: DDPM vs DDIM vs accelerated methods
4. **Guidance Analysis**: Effect of classifier-free guidance strength
5. **Scale Study**: Performance vs model size and training time

## Research Connections

### Foundational Papers
1. Ho et al. (2020) - "Denoising Diffusion Probabilistic Models"
2. Song et al. (2021) - "Denoising Diffusion Implicit Models"
3. Dhariwal & Nichol (2021) - "Diffusion Models Beat GANs on Image Synthesis"
4. Ho & Salimans (2022) - "Classifier-Free Diffusion Guidance"

### Theoretical Foundations
1. Song et al. (2021) - "Score-Based Generative Modeling through SDEs"
2. Anderson (1982) - "Reverse-time diffusion equation models"
3. Sohl-Dickstein et al. (2015) - "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"

### Modern Applications
1. Ramesh et al. (2022) - "Hierarchical Text-Conditional Image Generation with CLIP Latents" (DALL-E 2)
2. Saharia et al. (2022) - "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding" (Imagen)
3. Rombach et al. (2022) - "High-Resolution Image Synthesis with Latent Diffusion Models" (Stable Diffusion)

## Resources

### Primary Sources
1. **Ho et al. (2020)** - Original DDPM paper
2. **Song et al. (2021)** - Score-based perspective
3. **Yang Song's Blog** - "Generative Modeling by Estimating Gradients of the Data Distribution"

### Video Resources
1. **Yannic Kilcher - DDPM Explained**
   - Clear mathematical explanation
2. **Outlier - Diffusion Models Explained**
   - Intuitive understanding
3. **Ari Seff - Diffusion Models from Scratch**
   - Implementation details

### Software Resources
1. **Hugging Face Diffusers**: Production-ready implementations
2. **OpenAI DALL-E 2**: API access
3. **Stability AI**: Open-source Stable Diffusion

## Socratic Questions

### Understanding
1. Why do diffusion models generate higher quality images than GANs for many tasks?
2. How does the noise schedule affect sample quality and training dynamics?
3. What's the connection between diffusion models and score-based generative models?

### Extension
1. How would you design a diffusion model for non-Euclidean data (graphs, manifolds)?
2. Can you derive the continuous-time limit of the diffusion process?
3. What happens to diffusion models in very high dimensions?

### Research
1. How can we make diffusion models sample faster while maintaining quality?
2. What new conditioning mechanisms might improve controllable generation?
3. How do diffusion models compare to other generative models in terms of mode coverage and sample quality?

## Exercises

### Theoretical
1. Derive the forward process closed-form formula
2. Prove the equivalence between DDPM and score-based models
3. Analyze the effect of different noise schedules on training dynamics

### Implementation
1. Build DDPM from scratch for MNIST or CIFAR-10
2. Implement DDIM sampling for faster generation
3. Add classifier-free guidance for conditional generation
4. Create evaluation pipeline with FID and IS metrics

### Research
1. Study the relationship between sample quality and number of diffusion steps
2. Investigate novel architectures beyond U-Net for diffusion models
3. Explore applications of diffusion models to your domain of interest

## Advanced Topics

### Stochastic Differential Equations
- **Continuous formulation**: Diffusion as SDE solution
- **Ito vs Stratonovich**: Different SDE formulations
- **Numerical methods**: Discretization schemes for SDEs

### Optimal Transport Perspective
- **SchrÃ¶dinger bridge**: Connecting diffusion to optimal transport
- **Flow matching**: Alternative to diffusion for generation
- **Wasserstein geodesics**: Optimal paths between distributions

### Accelerated Sampling
- **Progressive distillation**: Multi-step to few-step models
- **Consistency models**: Direct mapping from noise to data
- **Flow-based acceleration**: Using normalizing flows for speed